<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="Images\chart.png" />
  <link rel="stylesheet" href="https://pro.fontawesome.com/releases/v5.10.0/css/all.css" />
  <link rel="stylesheet"
    href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" />
  <link rel="stylesheet" href="app\style.css">
  <title>Machine Learning</title>
</head>

<body>
  <div class="container">
    <nav class="navbar">
      <h1 class="navbar-head"><i class="fas fa-gamepad"></i> Machine Learning with Video Games
      </h1>
    </nav>
    <div class="container">
      <script
        src="https://gist.github.com/cheyroseflammer/2183e5c6175a6216789307a57d79430c"></script>
    </div>
    <div class="container">
      <h2>Data Visualization Graphs</h2>
    </div>
    <div class="row">
      <div class="column">
        <img src="Images\before.svg" alt="developer count chart">
        <img src="Images\after.svg" alt="publisher count chart">
      </div>
    </div>
    <div class="container">
      <h2>Objective</h2>
      <br>
      <p>This project consisted of building a machine learning model to predict whether or not a video game 
        would be considered a "hit." </p>
      <div class="cont">
        <div class="card">
          <div class="box">
            <div class="text">Exploring Data</div>
            <p>
              Data exploration began by finding a video game sales dataset on Kaggle.com. 
              We explored the data using pandas to read our csv into a dataframe and viewed the first 5
              results using head() to give us an idea of what our data looked like. 
              The dataset included the following features: Name, Platform, Year of Release, Genre, Publisher, 
              NA Sales, EU Sales, JP Sales, Other Sales, Global Sales, Critic Score, Critic Count, User Score, 
              User Count, Developer, and Rating
              Exploration began by creating scatter charts to visualize features of interest; 
              which included critic score versus global sales. Data exploration cotinued by loading the 
              data into a Tableau workbook. Once loaded, various graphs were made to determine 
              relationships between each of the additional features available in the dataset.
            </p>
          </div>
        </div>
        <div class="card">
          <div class="box">
            <div class="text">Preprocessing</div>
            <p>
              Following exploration, the dataset was read into a Jupyter notebook to begin preprocessing.
              The first step in preprocessing included removing any outliers within the data.
              Once outliers were removed, the data was checked to determine the ratio of null values within each 
              column. To reduce the number of null values, the data as whole itself was reduced to reflect only 
              prominent video game platforms. The data was then checked again to determine the ratio of null values 
              within each column. 
              The ratio of null values within several key variables was assessed once more by replacing null 
              values with either median or mode values. Once null values were taken care of, it was time to drop 
              unecessary columns. With an almost clean dataset, the next step was to define what 
              would be considered a "hit" video game. This was done using a function that defined a video game as 
              a "hit" if the game's global sales were equal to or above 10 million sales. The function then returned 
              either a "1" or a "0" if the game was a "hit" ot not, respectively.
            </p>
          </div>
        </div>
        <div class="card">
          <div class="box">
            <div class="text">Building the Models</div>
            <p>
              With a clean dataset and a video game "hit" defined, it was time to start building our models.
              This proces began by defining X and Y values and splitting these into training and testng data.
              The first model ran was a Logistic Regression Model. 
              This model returned a training score of 0.9918 and a testing score of 0.9918.
              The second model ran was a Random Forest Classifier model.
              Thi model returned a training score of 1.0 and a testing score 0.9942.
              Both models were erroneously overfitting, and so to account this, the data was scaled and 
              normalized. Additional features were also removed to reduce the noise within the data.
              Despite steps taken to refine both models, the training and testing scores remained the same.
            </p>
          </div>
        </div>
        <div class="card">
          <div class="box">
            <div class="text">Considerations</div>
            <p>
              While we could not achieve meaningful predicitive power with either model, we did step away 
              with significant considerations.
              The first being that perhaps a feature other than global sales may have produced better 
              predictive power. Features such as Critic Score or User Score, that can have a fairly large 
              effect on whether a game is bought or played, and thus considered a "hit."
              The second consideration was that perhaps a third different model would have performed better. 
              A neural net machine learning model may have been the approach to instead take. We were under 
              the assumption a simpler model would outperform, however a multilayer network may have better 
              classified and predicted a game as a "hit."
              Overall, the process of finding and preparing a dataset to build each machine learning model 
              was quite the endeavor!
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
  </div>
</body>

</html>